{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d008ee-0538-477e-b610-9469dedf23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from pvlive_api import PVLive\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import openmeteo_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38dfd609-99e5-4279-bf3f-279998525531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating PVLive API as per GIT repo instructions: https://github.com/SheffieldSolar/PV_Live-API\n",
    "pvl = PVLive(\n",
    "    retries=3, # Optionally set the number of retries when intermittent issues are encountered\n",
    "    proxies=None, # Optionally pass a dict of proxies to use when making requests\n",
    "    ssl_verify=True, # Optionally disable SSL certificate verification (not advised!)\n",
    "    domain_url=\"api.pvlive.uk\", # Optionally switch between the prod and FOF APIs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12bc7e09-55f2-4a19-8a7f-022b159fd24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GSP and timeframes to call functions\n",
    "start = datetime(2025, 1, 1, 0, 0, tzinfo=pytz.UTC)\n",
    "end = datetime(2025, 12, 31, 23, 30, tzinfo=pytz.UTC)\n",
    "gsp_id = 12  # select GSP ID to extract data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd8f003-997b-4437-9948-1c9393e83042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mwp(region=\"gsp\", include_history=True):\n",
    "    \"\"\"\n",
    "    Load and return the MWp deployment dataframe as mwp_df via pvl.deployment.\n",
    "    \"\"\"\n",
    "    mwp_df = pvl.deployment(region=region, include_history=include_history)\n",
    "    return mwp_df\n",
    "\n",
    "def load_gsp(gsp_path=\"../data/gsp_info.csv\"):\n",
    "    \"\"\"\n",
    "    Load and return the GSP info dataframe as gsp_df from CSV,\n",
    "    filtered to only GSPs known to PVLive (pvl.gsp_ids).\n",
    "    \"\"\"\n",
    "    pvl = PVLive()\n",
    "    valid_ids = set(pvl.gsp_ids)\n",
    "\n",
    "    gsp_df = pd.read_csv(gsp_path)\n",
    "\n",
    "    if 'gsp_id' in gsp_df.columns:\n",
    "        # coerce non-numeric to NaN, drop those rows, cast to int, then filter by PVLive ids\n",
    "        gsp_df['gsp_id_num'] = pd.to_numeric(gsp_df['gsp_id'], errors='coerce')\n",
    "        gsp_df = gsp_df[gsp_df['gsp_id_num'].notna()].copy()\n",
    "        gsp_df['gsp_id_num'] = gsp_df['gsp_id_num'].astype(int)\n",
    "        gsp_df = gsp_df[gsp_df['gsp_id_num'].isin(valid_ids)].drop(columns=['gsp_id_num']).reset_index(drop=True)\n",
    "\n",
    "    return gsp_df\n",
    "\n",
    "def merge_gsp_location(mwp_df, gsp_df, gsp_col_mwp='GSPs', gsp_col_gsp='gsp_name'):\n",
    "    \"\"\"\n",
    "    Return a copy of mwp_df with columns gsp_lat, gsp_lon, region_name merged from gsp_df.\n",
    "    Matching is done case-insensitive and with whitespace stripped.\n",
    "    Remove the 'unkown' rows from the mwp_df - presumably misspelling of unknown.\n",
    "    Drop any rows with missing values.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Make copies to avoid mutating inputs\n",
    "    mwp = mwp_df.copy()\n",
    "    gsp = gsp_df.copy()\n",
    "\n",
    "    # Normalize join keys by aligning to string, stripping and putting in upper case\n",
    "    mwp['_gsp_key'] = mwp[gsp_col_mwp].astype(str).str.strip().str.upper()\n",
    "    gsp['_gsp_key'] = gsp[gsp_col_gsp].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Select only the columns we want to bring across (plus join key)\n",
    "    to_merge = gsp[['_gsp_key', 'gsp_id', 'gsp_lat', 'gsp_lon', 'region_name', 'pes_id']].drop_duplicates('_gsp_key')    \n",
    "    merged = mwp.merge(to_merge, on='_gsp_key', how='left') # Left merge so all mwp rows are kept\n",
    "    merged = merged.dropna(how='any')   # drop all rows where ther are NaN values - return only the 299 intersection GSPs\n",
    "    merged = merged[merged[gsp_col_mwp] != 'unkown']  # return the df where not equal to unkown\n",
    "    merged = merged.drop(columns=['_gsp_key'])  # Drop linking key\n",
    "\n",
    "    return merged\n",
    "    \n",
    "def gsp_locations(merged_df, gsp_col='GSPs'):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with one row per unique GSP containing\n",
    "    gsp_col, gsp_lat, gsp_lon, region_name, pes_id, and a combined\n",
    "    'GSP_region' column formatted \"GSPs | region_name\".\n",
    "    \"\"\"\n",
    "    gsp_locations_list = (\n",
    "        merged_df\n",
    "        .drop_duplicates(subset=[gsp_col])[[gsp_col, 'gsp_id', 'gsp_lat', 'gsp_lon', 'region_name', 'pes_id']]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    gsp_locations_list['GSP_region'] = gsp_locations_list[gsp_col].astype(str) + ' | ' + gsp_locations_list['region_name'].astype(str)\n",
    "    return gsp_locations_list\n",
    "\n",
    "def wide_cumul_capacity(merged_df, time_col='install_month', gsp_col='GSPs', value_col='cumul_capacity_mwp'):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with:\n",
    "    - one column for the time (time_col) monthly intervals\n",
    "    - one column per GSP (column name = GSP identifier)\n",
    "    - cells = value_col (cumulative capacity MWP)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    df = merged_df.copy()\n",
    "    try:\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "    except Exception:\n",
    "        pass\n",
    "    wide = df.pivot_table(index=time_col, columns=gsp_col, values=value_col, aggfunc='first')\n",
    "    wide = wide.reset_index()  # make time a regular column\n",
    "    return wide\n",
    "\n",
    "def download_generation_for_single_gsp(start, end, gsp_id, gsp_locations_list, include_national=False, extra_fields=\"\"):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with:\n",
    "    - generation data for selected period for one GSP\n",
    "    - one column for the time (time_col) HH intervals\n",
    "    - other columns for GSP identifiers\n",
    "    \"\"\"\n",
    "    valid_ids = gsp_locations_list['gsp_id'].dropna().astype(int).unique()  # Get the valid gsp_ids from gsp_locations_list\n",
    "    if gsp_id not in valid_ids:  # Check if the provided gsp_id is valid\n",
    "        return f\"Please select a GSP ID which appears in the GSP capacity list.\"\n",
    "\n",
    "    # Fetch data for the specific GSP ID using between function from PVLive API Class\n",
    "    generation_df = pvl.between(\n",
    "        start=start,\n",
    "        end=end,\n",
    "        entity_type=\"gsp\",\n",
    "        entity_id=int(gsp_id),\n",
    "        dataframe=True,\n",
    "        extra_fields=extra_fields\n",
    "    )\n",
    "\n",
    "    # Interpolate up to 12 consecutive NaN values in the generation output - 6 in either direction  \n",
    "    if generation_df is not None and not generation_df.empty:\n",
    "        generation_df['datetime_gmt'] = pd.to_datetime(generation_df['datetime_gmt'])\n",
    "        generation_df = generation_df.sort_values(['gsp_id', 'datetime_gmt']).set_index('datetime_gmt')\n",
    "        generation_df['generation_mw'] = (\n",
    "            generation_df.groupby('gsp_id')['generation_mw']\n",
    "            .apply(lambda s: s.interpolate(method='time', limit=12, limit_direction='both'))\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        generation_df = generation_df.reset_index()\n",
    "        \n",
    "    # Merge additional columns from gsp_locations_list\n",
    "    gsp_info = gsp_locations_list[gsp_locations_list['gsp_id'] == gsp_id]\n",
    "    if not gsp_info.empty:\n",
    "        # Merge on gsp_id to include other columns like gsp_lat, gsp_lon, etc.\n",
    "        generation_df = generation_df.merge(gsp_info, on='gsp_id', how='left')\n",
    "        \n",
    "    return generation_df\n",
    "\n",
    "def get_capacity_data_single_gsp(gsp_id, merged_df):\n",
    "    \"\"\"\n",
    "    Return a DataFrame containing capacity data for the specified GSP ID,\n",
    "    along with month and year columns based on the install_month\n",
    "    \n",
    "    \"\"\"\n",
    "    # Filter the DataFrame for the specified GSP ID and add time-series columns for mathing with generation df\n",
    "    capacity_data = merged_df[merged_df['gsp_id'] == gsp_id].copy()\n",
    "    capacity_data = capacity_data[['install_month', 'cumul_capacity_mwp', 'GSPs', 'gsp_lat', 'gsp_lon', 'region_name', 'pes_id']]   # Keep relevant columns\n",
    "    capacity_data['install_month'] = pd.to_datetime(capacity_data['install_month'])    # Convert install_month to datetime\n",
    "    capacity_data['month'] = capacity_data['install_month'].dt.month     # Create 'month' and 'year' columns\n",
    "    capacity_data['year'] = capacity_data['install_month'].dt.year\n",
    "    capacity_data['day'] = capacity_data['install_month'].dt.day\n",
    "    capacity_data['hour'] = capacity_data['install_month'].dt.hour\n",
    "    return capacity_data.reset_index(drop=True)\n",
    "\n",
    "def add_capacity_to_generation(generation_df, capacity_data):\n",
    "    \"\"\"\n",
    "    Returns dataframe which merges cumulative capacity and generation data for a single GSP\n",
    "    \n",
    "    \"\"\"\n",
    "    # Ensure datetime_gmt is in datetime format and extract month/year/day/hour\n",
    "    generation_df['datetime_gmt'] = pd.to_datetime(generation_df['datetime_gmt'])\n",
    "    generation_df['month'] = generation_df['datetime_gmt'].dt.month\n",
    "    generation_df['year'] = generation_df['datetime_gmt'].dt.year\n",
    "    generation_df['day'] = generation_df['datetime_gmt'].dt.day\n",
    "    generation_df['hour'] = generation_df['datetime_gmt'].dt.hour\n",
    "    \n",
    "    # Merge capacity data based on the month and year values\n",
    "    merged_df = generation_df.merge(\n",
    "        capacity_data[['month', 'year', 'cumul_capacity_mwp']], \n",
    "        on=['month', 'year'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    merged_df.rename(columns={'cumul_capacity_mwp': 'capacity_mwp'}, inplace=True)  # rename capacity column\n",
    "    merged_df['capacity_mwp'] = merged_df['capacity_mwp'].ffill() # replace last NaN values with final entry filling delayed capacity information\n",
    "    \n",
    "    return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a37ad703-6098-44ab-8f74-f39eac17ef41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_gmt</th>\n",
       "      <th>gsp_id</th>\n",
       "      <th>generation_mw</th>\n",
       "      <th>GSPs</th>\n",
       "      <th>gsp_lat</th>\n",
       "      <th>gsp_lon</th>\n",
       "      <th>region_name</th>\n",
       "      <th>pes_id</th>\n",
       "      <th>GSP_region</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>capacity_mwp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01 00:00:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.659577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-01 00:30:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.659577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-01 01:00:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.659577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-01 01:30:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.659577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-01 02:00:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.659577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17507</th>\n",
       "      <td>2025-12-31 21:30:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>12</td>\n",
       "      <td>2025</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>8.206667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17508</th>\n",
       "      <td>2025-12-31 22:00:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>12</td>\n",
       "      <td>2025</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>8.206667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17509</th>\n",
       "      <td>2025-12-31 22:30:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>12</td>\n",
       "      <td>2025</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>8.206667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17510</th>\n",
       "      <td>2025-12-31 23:00:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>12</td>\n",
       "      <td>2025</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>8.206667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17511</th>\n",
       "      <td>2025-12-31 23:30:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAMB_01</td>\n",
       "      <td>53.729427</td>\n",
       "      <td>-1.000941</td>\n",
       "      <td>Camblesforth</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CAMB_01 | Camblesforth</td>\n",
       "      <td>12</td>\n",
       "      <td>2025</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>8.206667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17512 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime_gmt  gsp_id  generation_mw     GSPs    gsp_lat  \\\n",
       "0     2025-01-01 00:00:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "1     2025-01-01 00:30:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "2     2025-01-01 01:00:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "3     2025-01-01 01:30:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "4     2025-01-01 02:00:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "...                         ...     ...            ...      ...        ...   \n",
       "17507 2025-12-31 21:30:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "17508 2025-12-31 22:00:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "17509 2025-12-31 22:30:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "17510 2025-12-31 23:00:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "17511 2025-12-31 23:30:00+00:00      12            0.0  CAMB_01  53.729427   \n",
       "\n",
       "        gsp_lon   region_name  pes_id              GSP_region  month  year  \\\n",
       "0     -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth      1  2025   \n",
       "1     -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth      1  2025   \n",
       "2     -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth      1  2025   \n",
       "3     -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth      1  2025   \n",
       "4     -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth      1  2025   \n",
       "...         ...           ...     ...                     ...    ...   ...   \n",
       "17507 -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth     12  2025   \n",
       "17508 -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth     12  2025   \n",
       "17509 -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth     12  2025   \n",
       "17510 -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth     12  2025   \n",
       "17511 -1.000941  Camblesforth    23.0  CAMB_01 | Camblesforth     12  2025   \n",
       "\n",
       "       day  hour  capacity_mwp  \n",
       "0        1     0      7.659577  \n",
       "1        1     0      7.659577  \n",
       "2        1     1      7.659577  \n",
       "3        1     1      7.659577  \n",
       "4        1     2      7.659577  \n",
       "...    ...   ...           ...  \n",
       "17507   31    21      8.206667  \n",
       "17508   31    22      8.206667  \n",
       "17509   31    22      8.206667  \n",
       "17510   31    23      8.206667  \n",
       "17511   31    23      8.206667  \n",
       "\n",
       "[17512 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call functions and create variables with dataframes for use in the app\n",
    "mwp_df = load_mwp() # loading the capacity df\n",
    "gsp_df = load_gsp() # loading the gsp locations df\n",
    "merged_df = merge_gsp_location(mwp_df, gsp_df) # merge capacity growth and locations\n",
    "gsp_locations_list = gsp_locations(merged_df) # merge capacity and locations without capacity growth over time\n",
    "generation_df = download_generation_for_single_gsp(start, end, gsp_id, gsp_locations_list) # generation df for selected gsp\n",
    "capacity_growth_all_gsps = wide_cumul_capacity(merged_df) # wide capacity growth df for all time and all gsps\n",
    "capacity_data_single_gsp = get_capacity_data_single_gsp(gsp_id, merged_df) # add month and year to capacity single gsp\n",
    "generation_and_capacity_single_gsp = add_capacity_to_generation(generation_df, capacity_data_single_gsp) # merged capacity and generation same time-series single gsp\n",
    "\n",
    "generation_and_capacity_single_gsp # extract the gsp generation and capacity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c9065-303f-4ec4-be2a-4dc0d6771474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
