{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d008ee-0538-477e-b610-9469dedf23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from pvlive_api import PVLive\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import openmeteo_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38dfd609-99e5-4279-bf3f-279998525531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating PVLive API as per GIT repo instructions: https://github.com/SheffieldSolar/PV_Live-API\n",
    "pvl = PVLive(\n",
    "    retries=3, # Optionally set the number of retries when intermittent issues are encountered\n",
    "    proxies=None, # Optionally pass a dict of proxies to use when making requests\n",
    "    ssl_verify=True, # Optionally disable SSL certificate verification (not advised!)\n",
    "    domain_url=\"api.pvlive.uk\", # Optionally switch between the prod and FOF APIs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12bc7e09-55f2-4a19-8a7f-022b159fd24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GSP and timeframes to call functions\n",
    "start = datetime(2025, 1, 1, 0, 0, tzinfo=pytz.UTC)\n",
    "end = datetime(2025, 12, 31, 23, 30, tzinfo=pytz.UTC)\n",
    "gsp_id = 12  # select GSP ID to extract data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd8f003-997b-4437-9948-1c9393e83042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mwp(region=\"gsp\", include_history=True):\n",
    "    \"\"\"\n",
    "    Load and return the MWp deployment dataframe as mwp_df via pvl.deployment.\n",
    "    \"\"\"\n",
    "    mwp_df = pvl.deployment(region=region, include_history=include_history)\n",
    "    return mwp_df\n",
    "\n",
    "def load_gsp(gsp_path=\"../data/gsp_info.csv\"):\n",
    "    \"\"\"\n",
    "    Load and return the GSP info dataframe as gsp_df from CSV,\n",
    "    filtered to only GSPs known to PVLive (pvl.gsp_ids).\n",
    "    \"\"\"\n",
    "    pvl = PVLive()\n",
    "    valid_ids = set(pvl.gsp_ids)\n",
    "\n",
    "    gsp_df = pd.read_csv(gsp_path)\n",
    "\n",
    "    if 'gsp_id' in gsp_df.columns:\n",
    "        # coerce non-numeric to NaN, drop those rows, cast to int, then filter by PVLive ids\n",
    "        gsp_df['gsp_id_num'] = pd.to_numeric(gsp_df['gsp_id'], errors='coerce')\n",
    "        gsp_df = gsp_df[gsp_df['gsp_id_num'].notna()].copy()\n",
    "        gsp_df['gsp_id_num'] = gsp_df['gsp_id_num'].astype(int)\n",
    "        gsp_df = gsp_df[gsp_df['gsp_id_num'].isin(valid_ids)].drop(columns=['gsp_id_num']).reset_index(drop=True)\n",
    "\n",
    "    return gsp_df\n",
    "\n",
    "def merge_gsp_location(mwp_df, gsp_df, gsp_col_mwp='GSPs', gsp_col_gsp='gsp_name'):\n",
    "    \"\"\"\n",
    "    Return a copy of mwp_df with columns gsp_lat, gsp_lon, region_name merged from gsp_df.\n",
    "    Matching is done case-insensitive and with whitespace stripped.\n",
    "    Remove the 'unkown' rows from the mwp_df - presumably misspelling of unknown.\n",
    "    Drop any rows with missing values.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Make copies to avoid mutating inputs\n",
    "    mwp = mwp_df.copy()\n",
    "    gsp = gsp_df.copy()\n",
    "\n",
    "    # Normalize join keys by aligning to string, stripping and putting in upper case\n",
    "    mwp['_gsp_key'] = mwp[gsp_col_mwp].astype(str).str.strip().str.upper()\n",
    "    gsp['_gsp_key'] = gsp[gsp_col_gsp].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Select only the columns we want to bring across (plus join key)\n",
    "    to_merge = gsp[['_gsp_key', 'gsp_id', 'gsp_lat', 'gsp_lon', 'region_name', 'pes_id']].drop_duplicates('_gsp_key')    \n",
    "    merged = mwp.merge(to_merge, on='_gsp_key', how='left') # Left merge so all mwp rows are kept\n",
    "    merged = merged.dropna(how='any')   # drop all rows where ther are NaN values - return only the 299 intersection GSPs\n",
    "    merged = merged[merged[gsp_col_mwp] != 'unkown']  # return the df where not equal to unkown\n",
    "    merged = merged.drop(columns=['_gsp_key'])  # Drop linking key\n",
    "\n",
    "    return merged\n",
    "    \n",
    "def gsp_locations(merged_df, gsp_col='GSPs'):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with one row per unique GSP containing\n",
    "    gsp_col, gsp_lat, gsp_lon, region_name, pes_id, and a combined\n",
    "    'GSP_region' column formatted \"GSPs | region_name\".\n",
    "    \"\"\"\n",
    "    gsp_locations_list = (\n",
    "        merged_df\n",
    "        .drop_duplicates(subset=[gsp_col])[[gsp_col, 'gsp_id', 'gsp_lat', 'gsp_lon', 'region_name', 'pes_id']]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    gsp_locations_list['GSP_region'] = gsp_locations_list[gsp_col].astype(str) + ' | ' + gsp_locations_list['region_name'].astype(str)\n",
    "    return gsp_locations_list\n",
    "\n",
    "def wide_cumul_capacity(merged_df, time_col='install_month', gsp_col='GSPs', value_col='cumul_capacity_mwp'):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with:\n",
    "    - one column for the time (time_col) monthly intervals\n",
    "    - one column per GSP (column name = GSP identifier)\n",
    "    - cells = value_col (cumulative capacity MWP)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    df = merged_df.copy()\n",
    "    try:\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "    except Exception:\n",
    "        pass\n",
    "    wide = df.pivot_table(index=time_col, columns=gsp_col, values=value_col, aggfunc='first')\n",
    "    wide = wide.reset_index()  # make time a regular column\n",
    "    return wide\n",
    "\n",
    "def download_generation_for_single_gsp(start, end, gsp_id, gsp_locations_list, include_national=False, extra_fields=\"\"):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with:\n",
    "    - generation data for selected period for one GSP\n",
    "    - one column for the time (time_col) HH intervals\n",
    "    - other columns for GSP identifiers\n",
    "    \"\"\"\n",
    "    valid_ids = gsp_locations_list['gsp_id'].dropna().astype(int).unique()  # Get the valid gsp_ids from gsp_locations_list\n",
    "    if gsp_id not in valid_ids:  # Check if the provided gsp_id is valid\n",
    "        return f\"Please select a GSP ID which appears in the GSP capacity list.\"\n",
    "\n",
    "    # Fetch data for the specific GSP ID using between function from PVLive API Class\n",
    "    generation_df = pvl.between(\n",
    "        start=start,\n",
    "        end=end,\n",
    "        entity_type=\"gsp\",\n",
    "        entity_id=int(gsp_id),\n",
    "        dataframe=True,\n",
    "        extra_fields=extra_fields\n",
    "    )\n",
    "\n",
    "    # Interpolate up to 12 consecutive NaN values in the generation output - 6 in either direction  \n",
    "    if generation_df is not None and not generation_df.empty:\n",
    "        generation_df['datetime_gmt'] = pd.to_datetime(generation_df['datetime_gmt'])\n",
    "        generation_df = generation_df.sort_values(['gsp_id', 'datetime_gmt']).set_index('datetime_gmt')\n",
    "        generation_df['generation_mw'] = (\n",
    "            generation_df.groupby('gsp_id')['generation_mw']\n",
    "            .apply(lambda s: s.interpolate(method='time', limit=12, limit_direction='both'))\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        generation_df = generation_df.reset_index()\n",
    "        \n",
    "    # Merge additional columns from gsp_locations_list\n",
    "    gsp_info = gsp_locations_list[gsp_locations_list['gsp_id'] == gsp_id]\n",
    "    if not gsp_info.empty:\n",
    "        # Merge on gsp_id to include other columns like gsp_lat, gsp_lon, etc.\n",
    "        generation_df = generation_df.merge(gsp_info, on='gsp_id', how='left')\n",
    "        \n",
    "    return generation_df\n",
    "\n",
    "def get_capacity_data_single_gsp(gsp_id, merged_df):\n",
    "    \"\"\"\n",
    "    Return a DataFrame containing capacity data for the specified GSP ID,\n",
    "    along with month and year columns based on the install_month\n",
    "    \n",
    "    \"\"\"\n",
    "    # Filter the DataFrame for the specified GSP ID and add time-series columns for mathing with generation df\n",
    "    capacity_data = merged_df[merged_df['gsp_id'] == gsp_id].copy()\n",
    "    capacity_data = capacity_data[['install_month', 'cumul_capacity_mwp', 'GSPs', 'gsp_lat', 'gsp_lon', 'region_name', 'pes_id']]   # Keep relevant columns\n",
    "    capacity_data['install_month'] = pd.to_datetime(capacity_data['install_month'])    # Convert install_month to datetime\n",
    "    capacity_data['month'] = capacity_data['install_month'].dt.month     # Create 'month' and 'year' columns\n",
    "    capacity_data['year'] = capacity_data['install_month'].dt.year\n",
    "    capacity_data['day'] = capacity_data['install_month'].dt.day\n",
    "    capacity_data['hour'] = capacity_data['install_month'].dt.hour\n",
    "    return capacity_data.reset_index(drop=True)\n",
    "\n",
    "def add_capacity_to_generation(generation_df, capacity_data):\n",
    "    \"\"\"\n",
    "    Returns dataframe which merges cumulative capacity and generation data for a single GSP\n",
    "    \n",
    "    \"\"\"\n",
    "    # Ensure datetime_gmt is in datetime format and extract month/year/day/hour\n",
    "    generation_df['datetime_gmt'] = pd.to_datetime(generation_df['datetime_gmt'])\n",
    "    generation_df['month'] = generation_df['datetime_gmt'].dt.month\n",
    "    generation_df['year'] = generation_df['datetime_gmt'].dt.year\n",
    "    generation_df['day'] = generation_df['datetime_gmt'].dt.day\n",
    "    generation_df['hour'] = generation_df['datetime_gmt'].dt.hour\n",
    "    \n",
    "    # Merge capacity data based on the month and year values\n",
    "    merged_df = generation_df.merge(\n",
    "        capacity_data[['month', 'year', 'cumul_capacity_mwp']], \n",
    "        on=['month', 'year'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    merged_df.rename(columns={'cumul_capacity_mwp': 'capacity_mwp'}, inplace=True)  # rename capacity column\n",
    "    merged_df['capacity_mwp'] = merged_df['capacity_mwp'].ffill() # replace last NaN values with final entry filling delayed capacity information\n",
    "    \n",
    "    return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37ad703-6098-44ab-8f74-f39eac17ef41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_mwp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Call functions and create variables with dataframes for use in the app\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mwp_df = \u001b[43mload_mwp\u001b[49m() \u001b[38;5;66;03m# loading the capacity df\u001b[39;00m\n\u001b[32m      3\u001b[39m gsp_df = load_gsp() \u001b[38;5;66;03m# loading the gsp locations df\u001b[39;00m\n\u001b[32m      4\u001b[39m merged_df = merge_gsp_location(mwp_df, gsp_df) \u001b[38;5;66;03m# merge capacity growth and locations\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_mwp' is not defined"
     ]
    }
   ],
   "source": [
    "# Call functions and create variables with dataframes for use in the app\n",
    "mwp_df = load_mwp() # loading the capacity df\n",
    "gsp_df = load_gsp() # loading the gsp locations df\n",
    "merged_df = merge_gsp_location(mwp_df, gsp_df) # merge capacity growth and locations\n",
    "gsp_locations_list = gsp_locations(merged_df) # merge capacity and locations without capacity growth over time\n",
    "generation_df = download_generation_for_single_gsp(start, end, gsp_id, gsp_locations_list) # generation df for selected gsp\n",
    "capacity_growth_all_gsps = wide_cumul_capacity(merged_df) # wide capacity growth df for all time and all gsps\n",
    "capacity_data_single_gsp = get_capacity_data_single_gsp(gsp_id, merged_df) # add month and year to capacity single gsp\n",
    "generation_and_capacity_single_gsp = add_capacity_to_generation(generation_df, capacity_data_single_gsp) # merged capacity and generation same time-series single gsp\n",
    "\n",
    "capacity_growth_all_gsps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c9065-303f-4ec4-be2a-4dc0d6771474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
